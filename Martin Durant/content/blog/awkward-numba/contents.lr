title: Vectorized nested data processing
---
body:


<img src="https://raw.githubusercontent.com/scikit-hep/awkward-1.0/main/docs-img/logo/logo-300px.png" alt="Data graph" width="350"/>

**TL;DR**: Hidden behind its general numpy API and particle physics history,
Awkward allows python users C-speed aggregations over arbitrarily nested data

## Scenario

We all know about dataframes and arrays/tensors. Data science examples rely on them almost exclusively,
and python has become the popular language it is in great part by the ease and speed with which pyData
packages can process this kind of data.

That's not all there is...

In fact there's a ton of "structured" data out there. Examples include, but are not limited to:
- JSON payloads of a REST service (or XML, or...)
- server logs (typically text with variable number and types of fields per line)
- all the documents in your favourite noSQL database
- avro/protobuf/thrift communication

Most data science tutorials will mention such data, along with the first spet to "clean" it, extracting some
subset that can be represented in tabular or array format. Why? Because processing nested data is hard, and
python has had no good tooling for it: if you have to create python lists and dictionaries for big data,
you very soon hit performance bottlenecks. However, binary storage formats like parquet do support efficient
*storage* of this type of data.

> Here, we take "structured" to mean the data has a defined schema, but that this includes nesting such as
> records, repeated elements (lists) and maps (dicts)

## Awkward

Award Array was built for this type of data. More specifically, it was made for the processing of
high-energy particle events, where each collision consists of many fragment particles, each of which,
in turn, consist of more, forming a complex tree of data. Actually, Awkward does a lot, including dispatching
to various hardware, and a whole suite of commands. One piece of this functionality, when I first saw it, seemed to
me that it could solve the nested-data problem at a stroke.

> Awkward is still in active development. You can install it with `pip` or `conda` as any other package,
> but building from source is a little more involved, due to the many compile-time dependencies.

## Example

Consider the following function, tp be applied to the [million songs](http://millionsongdataset.com/) dataset,
as transformed to parquet format. It collates a histogram of pitch intervals (i.e., difference in pitch from
one segment to the next) for each song of the dataset. There are total of 920,151,348 values of 
analysis.segment across all 1,000,000 songs, each of which is a struct containing (amongst other things)
a spectral histogram of the pitches in that segment. The total size of the dataset in parquet, after naive
conversion, is 78.2GB.

Here is a naive function to compute the histograms:

```python
@numba.njit
def collect_pitch_intervals(millionsongs):
    # output histogram
    pitch_intervals = np.zeros((len(millionsongs), 23), np.int64)
    # iterate over the songs with an index for the output
    for index, song in enumerate(millionsongs):
        previous = None
        # iterate over segments in a song (varies with length of song)
        for segment in song.analysis.segments:
            # segment.pitches is a spectrum over semitones (1D array of length 12)
            strongest_pitch = np.argmax(np.asarray(segment.pitches))
            if previous is not None:
                # histogram the changes in pitch (intervals)
                interval = strongest_pitch - previous
                if interval != 0:
                    pitch_intervals[index, interval + 11] += 1
            previous = strongest_pitch
    # return a histogram of intervals for each song
    return pitch_intervals
```

As discussed recently in time for the Dask Distributed Summit, this can be parallelised trivially with Dask,
assigning 1000 songs per partition

---
author: Martin Durant
---
pub_date: 2021-5-28
---
twitter_handle: martin_durant_
---
_hidden: yes
---
_discoverable: no
